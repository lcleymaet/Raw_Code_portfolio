{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# STAT 760 Homework 6\n",
        "## Lilly Peacor\n",
        "\n",
        "The below code will be a function that takes input of a dataset, a list where each entry is the number of neurons in that layer, number of entries per batch, number of epochs, accuracy level to stop early, and the activation functions for hidden layers and the output layer."
      ],
      "metadata": {
        "id": "mclkGZqh9HCY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title getting dataset from kaggle\n",
        "from google.colab import files\n",
        "#files.upload()\n",
        "#!mkdir ~/.kaggle\n",
        "!cp kaggle.json ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle.json"
      ],
      "metadata": {
        "id": "tpU-b84D1ocj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZAhHu5p89ARc"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "import time\n",
        "import math\n",
        "import sys\n",
        "import random\n",
        "\n",
        "def NN(X, Y, learning_rate = 0.01, epochs = 10, min_acc = 0.1, s1 = \"relu\", s2 = \"id\", layers = [2, 2, 2], batch_size = 100, print_diag = True, print_res = True):\n",
        "  \"\"\"\n",
        "  This function takes input of a dataset, assuming normalization and factorization has already occured.\n",
        "  The inputs are the input dataset, the output dataset, learning rate, the maximum number of epochs, the accuracy that\n",
        "    would cause the function to halt training, the activation function for all hidden layers, the\n",
        "    activation function for the output layer, a list of the number of neurons to include in each hidden layer -\n",
        "    this also shows the number of hidden layers via len(layers), the batch size for training, and whether or not to print the diagnostics\n",
        "\n",
        "  This function is not intended to surpass existing neural network functions in computing power or accuracy, but instead to give\n",
        "    better insight into how a fully-connected feed forward neural network is trained.\n",
        "\n",
        "  The function has several control points to ensure inputs are correct for the training process. If the inputs to\n",
        "    the function fail any checks, an error message will print to the terminal and the function will return null.\n",
        "    The checks include ensuring input and output are of same size, the batch size is not larger than the training data,\n",
        "    and if the selected activation functions are valid options.\n",
        "\n",
        "  The function will output a dictionary that contains the weight matrix 'w', the bias vectors 'b', the accuracy on train data 'train_acc',\n",
        "    the accuracy on test data 'test_acc', and the model output on the entire input dataset 'output'.\n",
        "\n",
        "  \"\"\"\n",
        "\n",
        "  #check activation functions\n",
        "  if s1 not in ['leaky_relu', 'relu', 'sigmoid', 'tanh'] or s2 not in ['leaky_relu', 'relu', 'sigmoid', 'tanh', 'id']:\n",
        "    print(\"Invalid activation function selected. Please choose from ['leaky_relu', 'relu', 'sigmoid', 'tanh', 'id]\")\n",
        "    return None\n",
        "\n",
        "  #check dataset lengths\n",
        "  if len(X) != len(Y):\n",
        "    print(\"Input and output datasets are not the same size.\\n\")\n",
        "    return None\n",
        "\n",
        "  #split into test and train sets using 70-30 split\n",
        "  X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3, random_state=2525)\n",
        "\n",
        "  #check if batch size is bigger than the size of the dataset. If it is, return null and print error message\n",
        "  if batch_size > len(X_train):\n",
        "    print(\"Batch size cannot be bigger than the size of the dataset.\\n\")\n",
        "    return None\n",
        "\n",
        "  if print_diag:\n",
        "    print(\"There are\", len(X_train), \"training samples and\", len(X_test), \"testing samples.\\n\")\n",
        "\n",
        "  #defining different activation functions and their derivatives\n",
        "  def relu(x):\n",
        "    return np.maximum(0, x)\n",
        "\n",
        "  def relu_grad(x):\n",
        "    if x > 0:\n",
        "      return 1\n",
        "    elif x == 0:\n",
        "      return 0.5\n",
        "    else:\n",
        "      return 0\n",
        "\n",
        "  def leaky_relu(x, alpha = 0.01):\n",
        "    return np.maximum(alpha * x, x)\n",
        "\n",
        "  def leaky_relu_grad(x, alpha = 0.01):\n",
        "    if x > 0:\n",
        "      return 1\n",
        "    elif x == 0:\n",
        "      return (1 + alpha) / 2\n",
        "    else:\n",
        "      return alpha\n",
        "\n",
        "  def sigmoid(x):\n",
        "    return 1 / (1 + math.exp(-x))\n",
        "\n",
        "  def sigmoid_grad(x):\n",
        "    return math.exp(-x) / (1 + math.exp(-x))**2\n",
        "\n",
        "  def tanh(x):\n",
        "    return math.tanh(x)\n",
        "\n",
        "  def tanh_grad(x):\n",
        "    return 1 / math.cosh(x)**2\n",
        "\n",
        "  def id(x):\n",
        "    return x\n",
        "\n",
        "  def id_grad(x):\n",
        "    return 1\n",
        "\n",
        "  #define the activation functions to use in calculations\n",
        "  def sig1(x):\n",
        "    if s1 == \"relu\":\n",
        "      return relu(x)\n",
        "    elif s1 == \"leaky_relu\":\n",
        "      return leaky_relu(x)\n",
        "    elif s1 == \"sigmoid\":\n",
        "      return sigmoid(x)\n",
        "    elif s1 == \"tanh\":\n",
        "      return tanh(x)\n",
        "    elif s1 == \"id\":\n",
        "      return id(x)\n",
        "\n",
        "  def sig1_vect(x):\n",
        "    return np.array([sig1(x[i]) for i in range(len(x))])\n",
        "\n",
        "  def sig1_grad(x):\n",
        "    if s1 == \"relu\":\n",
        "      return relu_grad(x)\n",
        "    elif s1 == \"leaky_relu\":\n",
        "      return leaky_relu_grad(x)\n",
        "    elif s1 == \"sigmoid\":\n",
        "      return sigmoid_grad(x)\n",
        "    elif s1 == \"tanh\":\n",
        "      return tanh_grad(x)\n",
        "    elif s1 == \"id\":\n",
        "      return id_grad(x)\n",
        "\n",
        "  def sig2(x):\n",
        "    if s2 == \"relu\":\n",
        "      return relu(x)\n",
        "    elif s2 == \"leaky_relu\":\n",
        "      return leaky_relu(x)\n",
        "    elif s2 == \"sigmoid\":\n",
        "      return sigmoid(x)\n",
        "    elif s2 == \"tanh\":\n",
        "      return tanh(x)\n",
        "    elif s2 == \"id\":\n",
        "      return id(x)\n",
        "\n",
        "  def sig2_vect(x):\n",
        "    return np.array([sig2(x[i]) for i in range(len(x))])\n",
        "\n",
        "  def sig2_grad(x):\n",
        "    if s2 == \"relu\":\n",
        "      return relu_grad(x)\n",
        "    elif s2 == \"leaky_relu\":\n",
        "      return leaky_relu_grad(x)\n",
        "    elif s2 == \"sigmoid\":\n",
        "      return sigmoid_grad(x)\n",
        "    elif s2 == \"tanh\":\n",
        "      return tanh_grad(x)\n",
        "    elif s2 == \"id\":\n",
        "      return id_grad(x)\n",
        "\n",
        "  #Some important numbers\n",
        "  m0 = np.shape(X)[1] #number of input variables\n",
        "\n",
        "  m_infty = 1 #number of output variables, this function currently only permits 1-d output\n",
        "\n",
        "  h = len(layers) #number of hidden layers\n",
        "\n",
        "  layers = [m0] + layers + [m_infty] #list of all layers and number of neurons\n",
        "\n",
        "  if print_diag:\n",
        "    for i, layer in enumerate(layers):\n",
        "      print(\"Layer\", i, \"has\", layer, \"neuron(s).\\n\")\n",
        "\n",
        "  #making and initializing the weight matrix\n",
        "  ##We randomly assign weights\n",
        "  ##There is a dummy row to make the \"real\" matrix index match with index in python\n",
        "  def init_w(layers):\n",
        "    weights = [list(np.zeros((1,1)))] #dummy weight matrix\n",
        "    for i in range(1, h + 2):\n",
        "      #each weight matrix is of size h_{i-1},h_i, weights assigned randomly\n",
        "      weights.append([[random.random() for _ in range(layers[i - 1])] for _ in range(layers[i])])\n",
        "    return weights\n",
        "\n",
        "  weights = init_w(layers)\n",
        "\n",
        "  def init_b(layers):\n",
        "    biases = [list(np.zeros(1))] #dummy bias vector\n",
        "    for i in range(1, h + 2):\n",
        "      biases.append([0 for _ in range(layers[i])]) #initialize biases at 0\n",
        "    return biases\n",
        "\n",
        "  biases = init_b(layers)\n",
        "\n",
        "  if print_diag:\n",
        "    for i in range(1,h+2):\n",
        "      print(\"layer\",i)\n",
        "      print(\"Weight matrix dimension is\",layers[i],\"x\",layers[i-1],\":\")\n",
        "      print(weights[i])\n",
        "      print(\"Bias vector dimension is\",layers[i],\":\")\n",
        "      print(biases[i])\n",
        "      print(\"\\n\")\n",
        "\n",
        "  #Batching\n",
        "\n",
        "  #list of all indices in Training set\n",
        "  indices = [i for i in range(len(X_train))]\n",
        "\n",
        "  #function to create a set of batches for each epoch, the batches will contain only indices of the original set of data\n",
        "  def batch_gen(batch_size, indices):\n",
        "    batches = [] #list of batches that will contain a list of indices for each batch\n",
        "    random.shuffle(indices) #scramble the list of indices\n",
        "    for i in range(math.floor(len(indices) / batch_size)):\n",
        "      if i != math.floor(len(indices) / batch_size) - 1:\n",
        "        batches.append(indices[i * batch_size : (i + 1) * batch_size])\n",
        "      else:\n",
        "        #final batch may be smaller than batch_size, this line avoids sampling out of bounds\n",
        "        batches.append(indices[i * batch_size : ])\n",
        "    return batches\n",
        "\n",
        "  if print_diag:\n",
        "    print(\"There are\", math.ceil(len(X_train) / batch_size), \"batches in each epoch.\\n\")\n",
        "\n",
        "  #Function to calculate output of each layer\n",
        "  def k_out(input, k, weights, biases):\n",
        "    #if in input layer, don't do anything\n",
        "    if k == 0:\n",
        "      return input, input #2-D output for before and after activation function\n",
        "    #initialize v with input layer\n",
        "    v = np.array(input)\n",
        "    #loop over each layer to arrive at output of layer k\n",
        "    for i in range(1,k+1):\n",
        "      #grab weight matrix and bias vector for this layer\n",
        "      W = np.array(weights[i])\n",
        "      b = np.array(biases[i])\n",
        "      #compute u - Weights*v{i-1}+biases\n",
        "      u= W@v + b\n",
        "      #if the layer is not the output layer, use sig1, else use sig2\n",
        "      if i < h+1:\n",
        "        v = np.array(sig1_vect(u))\n",
        "      else:\n",
        "        v = np.array(sig2_vect(u))\n",
        "      #if print_diag:\n",
        "      #  print(\"Layer\", i, \"output is\", v)\n",
        "    return u, v\n",
        "\n",
        "  #diagnostic print to ensure the above funciton is working as intended\n",
        "  #if print_diag:\n",
        "  #  #randomly generate some vector of size m0\n",
        "  #  x = np.array([random.random() for _ in range(m0)])\n",
        "  #  for k in range(h+2):\n",
        "  #    print(\"Output of layer\", k, \"is\", k_out(x, k, weights, biases)[1],\"\\n\")\n",
        "\n",
        "  #We will use a loss function of MSE here\n",
        "\n",
        "  #We take the various partial derivatives of the loss function for gradient descent\n",
        "\n",
        "  #w.r.t. v\n",
        "  def dldv(B,k,i,s,weights,biases,layers):\n",
        "    #This partial derivative is only used up to the last hidden layer, not the\n",
        "    # output layer\n",
        "    if k == h: #if in final hidden layer\n",
        "      f1 = 2 * (k_out(X_train[i], h + 1, weights, biases)[1][0] - Y_train[i]) / len(B)\n",
        "      f2 = sig2_grad(k_out(X_train[i], h + 1, weights, biases)[0][0])\n",
        "      f3 = weights[h+1][0][s]\n",
        "      return(f1 * f2 * f3)\n",
        "    elif k < h: #if in hidden layer not the last one\n",
        "      return(sum([dldv(B, k + 1, i, r, weights, biases, layers) * sig1_grad(k_out(X_train[i], k + 1, weights, biases)[0][r - 1]) * weights[k + 1][r][s] for r in range(layers[k + 1])]))\n",
        "    else: #if not in hidden layers print error and return null\n",
        "      print(\"k out of bounds for dldv, returning null\")\n",
        "      return None\n",
        "\n",
        "  #w.r.t. w\n",
        "  def dldw(B, k, r, s, weights, biases, layers):\n",
        "    #output layer action\n",
        "    if k == h+1:\n",
        "      return(2 * sum([(k_out(X_train[i], h + 1, weights, biases)[1][0] - Y_train[i]) * sig2_grad(k_out(X_train[i], h + 1, weights, biases)[0][0]) * k_out(X_train[i], h, weights, biases)[1][s] for i in B]) / len(B))\n",
        "    #hidden layer action\n",
        "    if k < h+1:\n",
        "      return(sum([dldv(B, k, i, r, weights, biases, layers) * sig1_grad(k_out(X_train[i], k, weights, biases)[0][r]) * k_out(X_train[i], k - 1, weights, biases)[1][s] for i in B]))\n",
        "    #error for out of bounds k\n",
        "    if k > h + 1:\n",
        "      print(\"k out of bounds for dldw, returning null\")\n",
        "      return None\n",
        "\n",
        "  #w.r.t. b\n",
        "  def dldb(B, k, r, weights, biases, layers):\n",
        "    #output layer action\n",
        "    if k == h+1:\n",
        "      return(sum([(k_out(X_train[i], h + 1, weights, biases)[1][0] - Y_train[i]) * sig2_grad(k_out(X_train[i], h + 1, weights, biases)[0][0]) for i in B]))\n",
        "    #hidden layer action\n",
        "    if k < h + 1:\n",
        "      return(sum([dldv(B, k, i, r, weights, biases, layers) * sig1_grad(k_out(X_train[i], k, weights, biases)[0][r]) for i in B]))\n",
        "    #error for out of bounds k\n",
        "    if k > h + 1:\n",
        "      print(\"k out of bounds for dldb, returning null\")\n",
        "      return None\n",
        "\n",
        "\n",
        "  #The calculation of mse to be used to track overall accuracy on train and test data\n",
        "  def MSE(X, Y, weights, biases):\n",
        "    mse = 0\n",
        "    for i in range(len(X)):\n",
        "      mse += (k_out(X[i], h + 1, weights, biases)[1][0] - Y[i])**2\n",
        "    return mse / len(X)\n",
        "\n",
        "  #training the model via gradient descent\n",
        "  if print_res:\n",
        "    print(\"Model training in progress...\")\n",
        "    t1 = time.time()\n",
        "\n",
        "  #lists to store accuracies throughout training, stores an entry each epoch\n",
        "  ##These contain the MSE prior to training\n",
        "  train_acc = [MSE(X_train, Y_train, weights, biases)]\n",
        "  test_acc = [MSE(X_train, Y_train, weights, biases)]\n",
        "\n",
        "  #count variable to track epochs\n",
        "  epoch_num = 0\n",
        "\n",
        "  #dummy variable to start off change in accuracy\n",
        "  d_mse = min_acc + 10.0 #min_acc+10 ensure is larger than minimum accuracy change set\n",
        "\n",
        "  #loop exits when either max number of epochs is reached or the change in mse is less than min_acc\n",
        "  while epoch_num < epochs and d_mse > min_acc:\n",
        "    #we re-batch each epoch randomly\n",
        "    if print_diag:\n",
        "      print(f\"Epoch {epoch_num + 1} of {epochs}\\n\")\n",
        "    for i, B in enumerate(batch_gen(batch_size, indices)):\n",
        "      #loop over each batch\n",
        "      if print_diag:\n",
        "        print(f\"Batch {i + 1} of {math.ceil(len(X_train) / batch_size)}\\n\")\n",
        "\n",
        "      #work backwards through layers, starting at output layer ending at input layer\n",
        "      for k in range(h + 1, 0, -1):\n",
        "        #loop over each neuron in layer k\n",
        "        for r in range(layers[k]):\n",
        "          #adjust biases for each neuron, replacing old set of biases\n",
        "          biases[k][r] -= learning_rate * dldb(B, k, r, weights, biases, layers)\n",
        "          #loop over each input to neuron r in layer k\n",
        "          for s in range(layers[k - 1]):\n",
        "            #adjust weights for each input into neuron s\n",
        "            weights[k][r][s] -= learning_rate * dldw(B, k, r, s, weights, biases, layers)\n",
        "\n",
        "    #add new mse to the lists\n",
        "    train_acc.append(MSE(X_train, Y_train, weights, biases))\n",
        "    test_acc.append(MSE(X_test, Y_test, weights, biases))\n",
        "    #alter change in accuracy\n",
        "    d_mse = abs(train_acc[epoch_num + 1] - train_acc[epoch_num])\n",
        "    #iterate the epoch number\n",
        "    epoch_num += 1\n",
        "\n",
        "  if print_res:\n",
        "    t2 = time.time()\n",
        "    print(f\"Training took {round(t2 - t1, 2)} seconds to complete\")\n",
        "    print(f\"Minimum change in MSE reached after {epoch_num - 1}\")\n",
        "    print(f\"Final MSE for training data is {train_acc[-1]}\")\n",
        "    print(f\"Final MSE for test data is {test_acc[-1]}\")\n",
        "\n",
        "  #Get model output on the full entered dataset\n",
        "  NN_out = []\n",
        "  for i in range(len(X)):\n",
        "    NN_out.append(k_out(X[i], h + 1, weights, biases)[1][0])\n",
        "\n",
        "  #create output dictionary\n",
        "  output = {\"w\": weights, \"b\": biases, \"train_acc\": train_acc[-1], \"test_acc\": test_acc[-1], \"Output\": NN_out}\n",
        "\n",
        "  return output\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#import data\n",
        "import kagglehub\n",
        "# Download latest version\n",
        "path = kagglehub.dataset_download(\"akshaydattatraykhare/data-for-admission-in-the-university\")\n",
        "\n",
        "#print(\"Path to dataset files:\", path)\n",
        "\n",
        "data = pd.read_csv('/kaggle/input/data-for-admission-in-the-university/adm_data.csv')\n",
        "\n",
        "#drop Serial No. column\n",
        "data = data.drop('Serial No.', axis=1)\n",
        "\n",
        "#inspect dataset\n",
        "data.info()\n",
        "\n",
        "#split to X and Y\n",
        "X = data.drop('Chance of Admit ', axis=1)\n",
        "Y = data['Chance of Admit ']\n",
        "\n",
        "#normalize non-categorical variables\n",
        "for col in X.columns:\n",
        "  if col != 'Research' and col != 'University Rating':\n",
        "    X[col] = (X[col] - X[col].mean()) / X[col].std()\n",
        "  else:\n",
        "    #OHE the categorical variables\n",
        "    X = pd.get_dummies(X, columns=[col])\n",
        "\n",
        "for col in X.columns:\n",
        "  #convert to numeric for boolean variables\n",
        "  if \"University\" in col or \"Research\" in col:\n",
        "    X[col] = X[col].astype(int)\n",
        "\n",
        "#reinspect X\n",
        "X.info()\n",
        "X.head()\n",
        "\n",
        "#convert X and Y to numpy\n",
        "X = np.array(X)\n",
        "Y = np.array(Y)\n",
        "\n",
        "#run the neural network function\n",
        "#model_out = NN(X, Y, learning_rate = 0.01, epochs = 10, min_acc = 0.01, s1 = \"relu\", s2 = \"id\", layers = [2, 3, 4, 3], batch_size = 50, print_diag = True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FNuGyjqNxXhp",
        "outputId": "bce60da1-06c9-413e-de80-2457efb6f86d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 400 entries, 0 to 399\n",
            "Data columns (total 8 columns):\n",
            " #   Column             Non-Null Count  Dtype  \n",
            "---  ------             --------------  -----  \n",
            " 0   GRE Score          400 non-null    int64  \n",
            " 1   TOEFL Score        400 non-null    int64  \n",
            " 2   University Rating  400 non-null    int64  \n",
            " 3   SOP                400 non-null    float64\n",
            " 4   LOR                400 non-null    float64\n",
            " 5   CGPA               400 non-null    float64\n",
            " 6   Research           400 non-null    int64  \n",
            " 7   Chance of Admit    400 non-null    float64\n",
            "dtypes: float64(4), int64(4)\n",
            "memory usage: 25.1 KB\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 400 entries, 0 to 399\n",
            "Data columns (total 12 columns):\n",
            " #   Column               Non-Null Count  Dtype  \n",
            "---  ------               --------------  -----  \n",
            " 0   GRE Score            400 non-null    float64\n",
            " 1   TOEFL Score          400 non-null    float64\n",
            " 2   SOP                  400 non-null    float64\n",
            " 3   LOR                  400 non-null    float64\n",
            " 4   CGPA                 400 non-null    float64\n",
            " 5   University Rating_1  400 non-null    int64  \n",
            " 6   University Rating_2  400 non-null    int64  \n",
            " 7   University Rating_3  400 non-null    int64  \n",
            " 8   University Rating_4  400 non-null    int64  \n",
            " 9   University Rating_5  400 non-null    int64  \n",
            " 10  Research_0           400 non-null    int64  \n",
            " 11  Research_1           400 non-null    int64  \n",
            "dtypes: float64(5), int64(7)\n",
            "memory usage: 37.6 KB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Trying with varying the activation function since those settings appear to be great\n",
        "model_out2 = NN(X, Y, learning_rate = 0.01, epochs = 10, min_acc = 0.005, s1 = \"relu\", s2 = \"id\", layers = [2, 4, 6, 3], batch_size = 50, print_diag = True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hXRaqJd7HSkG",
        "outputId": "d553b79d-3471-4b64-e1a6-5cc0955d9705"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "There are 280 training samples and 120 testing samples.\n",
            "\n",
            "Layer 0 has 12 neuron(s).\n",
            "\n",
            "Layer 1 has 2 neuron(s).\n",
            "\n",
            "Layer 2 has 4 neuron(s).\n",
            "\n",
            "Layer 3 has 4 neuron(s).\n",
            "\n",
            "Layer 4 has 3 neuron(s).\n",
            "\n",
            "Layer 5 has 1 neuron(s).\n",
            "\n",
            "layer 1\n",
            "Weight matrix dimension is 2 x 12 :\n",
            "[[0.5470201068802534, 0.9936752395350533, 0.7505703831298098, 0.6657461866604492, 0.7458062617943696, 0.5143085749336186, 0.5046618965039839, 0.1654390467734118, 0.6638407346289795, 0.17500578749862294, 0.45629361785028155, 0.37376238257657823], [0.4537065184929485, 0.41633745697557456, 0.16583883621423867, 0.5469990721656077, 0.6894522286705589, 0.050718394575320125, 0.651613813879537, 0.1989227667776362, 0.6458168194676914, 0.6118079811570232, 0.17151372489278094, 0.03229034116418472]]\n",
            "Bias vector dimension is 2 :\n",
            "[0, 0]\n",
            "\n",
            "\n",
            "layer 2\n",
            "Weight matrix dimension is 4 x 2 :\n",
            "[[0.5599987166945318, 0.26998851135313895], [0.2625634618698178, 0.052945586284560364], [0.2874032363021888, 0.4561960974960525], [0.0027717731151478686, 0.41325949578621024]]\n",
            "Bias vector dimension is 4 :\n",
            "[0, 0, 0, 0]\n",
            "\n",
            "\n",
            "layer 3\n",
            "Weight matrix dimension is 4 x 4 :\n",
            "[[0.16525896273845397, 0.4265929184917949, 0.4948123054437795, 0.08485297741404219], [0.14632742529660492, 0.5807889498040015, 0.5742354569185839, 0.025940080593650272], [0.9458411820335271, 0.43813817813807865, 0.8635586742361105, 0.44286808470371153], [0.03381864628169462, 0.3706075013465401, 0.9568004290605466, 0.4350241262076022]]\n",
            "Bias vector dimension is 4 :\n",
            "[0, 0, 0, 0]\n",
            "\n",
            "\n",
            "layer 4\n",
            "Weight matrix dimension is 3 x 4 :\n",
            "[[0.15727831472146314, 0.12373222653760951, 0.2733179519255988, 0.8390712748835255], [0.8219379690827465, 0.1303109488348686, 0.8984563156749642, 0.33585936953692963], [0.04253460122955621, 0.8569245618435192, 0.5965219292057561, 0.11766533054984829]]\n",
            "Bias vector dimension is 3 :\n",
            "[0, 0, 0]\n",
            "\n",
            "\n",
            "layer 5\n",
            "Weight matrix dimension is 1 x 3 :\n",
            "[[0.4052806036787183, 0.06969888810816793, 0.6039899288871478]]\n",
            "Bias vector dimension is 1 :\n",
            "[0]\n",
            "\n",
            "\n",
            "There are 6 batches in each epoch.\n",
            "\n",
            "Model training in progress...\n",
            "Epoch 1 of 10\n",
            "\n",
            "Batch 1 of 6\n",
            "\n",
            "Batch 2 of 6\n",
            "\n",
            "Batch 3 of 6\n",
            "\n",
            "Batch 4 of 6\n",
            "\n",
            "Batch 5 of 6\n",
            "\n",
            "Epoch 2 of 10\n",
            "\n",
            "Batch 1 of 6\n",
            "\n",
            "Batch 2 of 6\n",
            "\n",
            "Batch 3 of 6\n",
            "\n",
            "Batch 4 of 6\n",
            "\n",
            "Batch 5 of 6\n",
            "\n",
            "Epoch 3 of 10\n",
            "\n",
            "Batch 1 of 6\n",
            "\n",
            "Batch 2 of 6\n",
            "\n",
            "Batch 3 of 6\n",
            "\n",
            "Batch 4 of 6\n",
            "\n",
            "Batch 5 of 6\n",
            "\n",
            "Training took 249.2 seconds to complete\n",
            "Minimum change in MSE reached after 2\n",
            "Final MSE for training data is 0.0075314229620745965\n",
            "Final MSE for test data is 0.008102426813198954\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Printing the best model, just with no diagnostics so that results are shown\n",
        "random.seed(2525)\n",
        "model_out = NN(X, Y, learning_rate = 0.01, epochs = 10, min_acc = 0.001, s1 = \"relu\", s2 = \"id\", layers = [3, 4, 4, 3], batch_size = 50, print_diag = False)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fry-l44icMrI",
        "outputId": "a522aa29-ecfe-49f7-a875-db61340aeef9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model training in progress...\n",
            "Training took 651.5 seconds to complete\n",
            "Minimum change in MSE reached after 4\n",
            "Final MSE for training data is 0.006250618414247497\n",
            "Final MSE for test data is 0.006643796622283756\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The best MSE I could achieve was $0.006250618414247497$ on train data and $0.006643796622283756$ on test data. This used a hidden layer structure of 3, 4, 4, 3 neurons. The model would at times produce errors with higher numbers of neurons. This function used ReLU as the hidden layer activation function and identity as the output layer activation function. Batch size was 50, learning rate of $0.01$, and was able to reach a change in MSE of $0.001$ after only 4 epochs, never reaching the maximum epochs specified of 10. This function appears to not have significant overfitting due to accuracy being relatively similar between test and train data."
      ],
      "metadata": {
        "id": "Gup003UY9FEo"
      }
    }
  ]
}